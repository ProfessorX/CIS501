
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
\documentclass[twocolumn]{article}


% Packages
% \usepackage{fancyhdr}
\usepackage[margin=10mm]{geometry}


% Housekeeping
\pagestyle{empty}

\begin{document}
\begin{itemize}
\item Categorical data: ordinal, nominal and binary.
\item Generative classifiers. Steps in modeling: model selection;
  density estimation of the data; classification of new data.
\item \textbf{Bayesian
    Theorem}. $p(c|x)=\frac{p(c,x)}{p(x)}=\frac{p(x|c)p(c)}{p(x)}$. $c$:
  the model to be inferred.  $x$: the observations. $p(x|c)$: the
  likelihood. $p(c)$ the prior. $p(c|x)$ the posterior. $p(x)$ the
  evidence.
\item Something more about Bayes. $p(c|x)$ means the probability of
  $c$ given $x$. Take a concrete example. $p(L|W)=0.75$ (from
  Wikipedia)---the probability of a woman with long hair is $75\%$. Or
  rather, the probability of event $L$ given event $W$ is $0.75$.
\item Even something more. $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$. $P(A)$
  the prior, the initial degree of belief in $A$. $P(A|B)$ the
  posterior, the degree of belief having accounted for $B$.
\item \textbf{Bayes decision rule}. $p(x)$ is independent of the class
  and $p(c)$ is frequently assumed to be the \emph{same} for all
  classes.
\item Standard \emph{k-NN}. When we use big $k$, then we could have a
  better noise resistance. At the same time we have worse resolution.
\item Kernel density estimation. When we increase the number of
  kernels, better smoothness is obtained. But we will have a much
  higher dimension. (You know what this implies.)
\item Calculate the joint probability distribution is normally very
  difficult, but Naive Bayesian method solves this by assuming that
  class conditional distributions are all
  independent. $p(c_{i}|x)=\frac{p(x_{1},x_{2},\ldots,x_{n}|c_{i})p(c_{i})}{p(x)}=\frac{p(x_{1}|c_{i})p(x_{2}|c_{i})\ldots
    p(x_{n}|c_{i})p(c_{i})}{p(x)}$.
\item Kind note about Naive Bayesian classifier. The evidence $p(x)$
  is class-independent, which means it has nothing to do with, the
  variable. Aka, the stuff that we \emph{should} really look into.
\item \textbf{Multivariate Bernoulli} model. For this model, one
  weakness is that whether a word (in Anti-Spamming case) appears 1 or
  100 times, the final probability representation is the same. (Let's
  do it in Chinese. It's NOT scientific.) 
\item \textbf{Multinomial} event model. For this model, each ``word''
  (in spamming example) is an event. One word appears, then it's
  probability is calculated. Appears many times, then times as many.
\item \textbf{Naive Bayesian Classifier for Continuous Data}. When we
  try to extend NBC to classify data with continuous data, there is
  one ``best practice'' that we should think about. The basic idea is
  to divide each feature into \emph{two or more} bins. 
\item Equal Frequency Discretization. In this method, each bin
  contains the same amount of points. (If you do want to ``imagine''
  what point is like, in hyper space.) It's analogous to \emph{k-NN}
  approach. 
\item \textbf{Fuzzy Discretization}.
\end{itemize}
\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
\documentclass[twocolumn]{article}


% Packages
% \usepackage{fancyhdr}
\usepackage[margin=10mm]{geometry}


% Housekeeping
\pagestyle{empty}

\begin{document}
\begin{itemize}
\item Categorical data: ordinal, nominal and binary.
\item Generative classifiers. Steps in modeling: model selection;
  density estimation of the data; classification of new data.
\item \textbf{Bayesian
    Theorem}. $p(c|x)=\frac{p(c,x)}{p(x)}=\frac{p(x|c)p(c)}{p(x)}$. $c$:
  the model to be inferred.  $x$: the observations. $p(x|c)$: the
  likelihood. $p(c)$ the prior. $p(c|x)$ the posterior. $p(x)$ the
  evidence.
\item Something more about Bayes. $p(c|x)$ means the probability of
  $c$ given $x$. Take a concrete example. $p(L|W)=0.75$ (from
  Wikipedia)---the probability of a woman with long hair is $75\%$. Or
  rather, the probability of event $L$ given event $W$ is $0.75$.
\item Even something more. $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$. $P(A)$
  the prior, the initial degree of belief in $A$. $P(A|B)$ the
  posterior, the degree of belief having accounted for $B$.
\item \textbf{Bayes decision rule}. $p(x)$ is independent of the class
  and $p(c)$ is frequently assumed to be the \emph{same} for all
  classes.
\item Life is never easy when you go abroad, alone.
\end{itemize}
\end{document}
